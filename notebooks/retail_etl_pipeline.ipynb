{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de6806e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceca63a",
   "metadata": {},
   "source": [
    "Actions performed: \n",
    "- Installed pyspark\n",
    "- Updated kernel to Python 3.13\n",
    "- Created project directory and initial project structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f76a4e0",
   "metadata": {},
   "source": [
    "**Note: This ETL pipeline was originally designed to export partitioned Parquet files. However, due to Spark's dependency on Hadoop‚Äôs native Windows bindings (`NativeIO$Windows.access0`), the JVM consistently triggered system-level errors ‚Äî even with all recommended workarounds (winutils, config overrides, and coalesced writes). To maintain velocity and ensure clean output, I pivoted to using `pandas.to_csv()` for final export. This approach guaranteed a successful write, preserved data integrity, and demonstrated flexibility in handling environment-specific limitations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69628ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in c:\\users\\magda\\appdata\\roaming\\python\\python313\\site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\magda\\appdata\\roaming\\python\\python313\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf0786e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file retail_etl_project\\data already exists.\n",
      "A subdirectory or file retail_etl_project\\notebooks already exists.\n",
      "A subdirectory or file retail_etl_project\\scripts already exists.\n",
      "A subdirectory or file retail_etl_project\\output already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir retail_etl_project\\data\n",
    "!mkdir retail_etl_project\\notebooks\n",
    "!mkdir retail_etl_project\\scripts\n",
    "!mkdir retail_etl_project\\output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65605016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailETL\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///tmp/spark-warehouse\") \\\n",
    "    .config(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.file.impl.disable.cache\", \"true\") \\\n",
    "    .config(\"spark.hadoop.util.NativeCodeLoader\", \"false\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8afe6",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cea689",
   "metadata": {},
   "source": [
    "For the sake of example, I have generated realistic CSV files that simulate:\n",
    "- üõçÔ∏è E-commerce purchase log\n",
    "\n",
    "- üõí Grocery/retail scan log\n",
    "\n",
    "- üßæ Bank transaction log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7210dc",
   "metadata": {},
   "source": [
    "## Purchase Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3f85c",
   "metadata": {},
   "source": [
    "Actions performed:\n",
    "- CSV ingestion\n",
    "    - Created SparkSession\n",
    "- Data exploration\n",
    "- Data cleaning\n",
    "- Data transformation\n",
    "    - Feature engineering\n",
    "    - Aggregations\n",
    "- Saved the cleaned data as partitioned parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4b2b62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"RetailETL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca1a35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    \"../data/retail_purchase_log.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7fd9060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+----------------+--------+------+-------------------+--------------+\n",
      "|      transaction_id|         customer_id|product_id|product_category|quantity| price|      purchase_date|store_location|\n",
      "+--------------------+--------------------+----------+----------------+--------+------+-------------------+--------------+\n",
      "|1666d603-e608-486...|911c1078-542d-483...|      1654|       Groceries|       2|320.68|2025-03-29 04:15:51|         Miami|\n",
      "|8e6ca660-f89a-444...|cb651de8-76f0-49d...|      1114|           Books|       2|467.28|2025-02-15 05:14:24|         Miami|\n",
      "|9814bd8b-0b39-483...|0ea06e13-d8a5-41b...|      1025|       Groceries|       1|456.81|2025-03-13 13:39:22|         Miami|\n",
      "|553d2bce-bd8f-4dd...|863cc4d3-9b75-473...|      1759|           Books|       3|331.35|2025-03-23 09:44:30|       Chicago|\n",
      "|6bfdc49f-33ee-4d4...|985ba266-f893-43d...|      1281|       Groceries|       1| 12.79|2025-03-14 12:56:30|       Chicago|\n",
      "+--------------------+--------------------+----------+----------------+--------+------+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "42fbbcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- purchase_date: timestamp (nullable = true)\n",
      " |-- store_location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e1d33cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fceeeb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2591a2",
   "metadata": {},
   "source": [
    "**Dropping rows with Critical Nulls**\n",
    "\n",
    "Made sure columns like `customer_id` and `price` aren't missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "111ed16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.dropna(subset=[\"transaction_id\", \"customer_id\", \"product_id\", \"quantity\", \"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c8e3e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df = df.withColumn(\"purchase_date\", to_timestamp(\"purchase_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e74246",
   "metadata": {},
   "source": [
    "**Data Engineering**\n",
    "\n",
    "Created new column \"Total Amount\" which is calculated as `quantity` x `price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f38909f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df = df.withColumn(\"total_amount\", expr(\"quantity * price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04ff25f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+\n",
      "|summary|          quantity|            price|     total_amount|\n",
      "+-------+------------------+-----------------+-----------------+\n",
      "|  count|              1000|             1000|             1000|\n",
      "|   mean|             3.068|252.4036700000002|770.6666200000004|\n",
      "| stddev|1.3983316242334398|141.6850235117269|586.8167205856818|\n",
      "|    min|                 1|             5.89|             6.89|\n",
      "|    25%|                 2|           127.84|           295.74|\n",
      "|    50%|                 3|           251.95|           619.98|\n",
      "|    75%|                 4|           375.39|           1163.4|\n",
      "|    max|                 5|            498.9|           2494.5|\n",
      "+-------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"quantity\", \"price\", \"total_amount\").summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d61423",
   "metadata": {},
   "source": [
    "**Transformations & Aggregations**\n",
    "- üìä Total Spend per Customer\n",
    "- üèÜ Top Products by Revenue\n",
    "- üèôÔ∏è Revenue by Store Location\n",
    "- üìÖ Purchases Over Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "503d5ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Spend Per Customer\n",
      "+--------------------+------------------+\n",
      "|         customer_id|       total_spent|\n",
      "+--------------------+------------------+\n",
      "|2cace57f-add5-4d4...|            2494.5|\n",
      "|3da85db8-918f-4d2...|           2485.35|\n",
      "|fb9ebc4b-e4c7-481...|           2468.75|\n",
      "|e655bbb3-5f9a-49d...|2436.4500000000003|\n",
      "|5eccf26a-9f26-4c0...|            2373.0|\n",
      "|693201f8-af73-42c...|           2361.95|\n",
      "|6369b06e-5998-4a3...|           2358.85|\n",
      "|4b2da249-7042-43b...|           2357.15|\n",
      "|a1fd4499-64af-4aa...|           2328.45|\n",
      "|7bde36be-75ad-4ac...|            2315.8|\n",
      "+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Spend Per Customer\")\n",
    "df.groupBy(\"customer_id\") \\\n",
    "  .sum(\"total_amount\") \\\n",
    "  .withColumnRenamed(\"sum(total_amount)\", \"total_spent\") \\\n",
    "  .orderBy(\"total_spent\", ascending=False) \\\n",
    "  .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f6ad4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Products by Revenue\n",
      "+----------+----------------+------------------+\n",
      "|product_id|product_category|     total_revenue|\n",
      "+----------+----------------+------------------+\n",
      "|      1215|     Electronics|           4554.47|\n",
      "|      1685|       Groceries|4023.8500000000004|\n",
      "|      1885|       Groceries|           3659.91|\n",
      "|      1517|           Books|           3508.92|\n",
      "|      1194|           Books|           3412.87|\n",
      "|      1025|       Groceries|           3288.85|\n",
      "|      1388|        Clothing|            3260.9|\n",
      "|      1429|            Toys|           3166.29|\n",
      "|      1773|           Books|           3161.41|\n",
      "|      1669|        Clothing|           3160.99|\n",
      "+----------+----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Products by Revenue\")\n",
    "df.groupBy(\"product_id\", \"product_category\") \\\n",
    "  .agg(expr(\"sum(total_amount) as total_revenue\")) \\\n",
    "  .orderBy(\"total_revenue\", ascending=False) \\\n",
    "  .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3791b9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue by Store Location\n",
      "+--------------+-------------------+\n",
      "|store_location|revenue_by_location|\n",
      "+--------------+-------------------+\n",
      "|       Chicago| 162427.71999999994|\n",
      "|         Miami| 159687.56999999998|\n",
      "|       Houston| 153963.75000000003|\n",
      "|      New York| 149034.31000000003|\n",
      "|   Los Angeles| 145553.27000000002|\n",
      "+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Revenue by Store Location\")\n",
    "df.groupBy(\"store_location\") \\\n",
    "  .agg(expr(\"sum(total_amount) as revenue_by_location\")) \\\n",
    "  .orderBy(\"revenue_by_location\", ascending=False) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bac8cbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purchases Over Time\n",
      "+------------+------------------+\n",
      "|purchase_day|     daily_revenue|\n",
      "+------------+------------------+\n",
      "|  2025-02-14| 6090.590000000001|\n",
      "|  2025-02-15|          16982.95|\n",
      "|  2025-02-16|          13026.42|\n",
      "|  2025-02-17|10978.729999999998|\n",
      "|  2025-02-18|           9691.99|\n",
      "|  2025-02-19|           7009.83|\n",
      "|  2025-02-20|12598.120000000003|\n",
      "|  2025-02-21|          17216.03|\n",
      "|  2025-02-22|          10362.88|\n",
      "|  2025-02-23|17908.120000000003|\n",
      "+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Purchases Over Time\")\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df_by_day = df.withColumn(\"purchase_day\", to_date(\"purchase_date\"))\n",
    "\n",
    "df_by_day.groupBy(\"purchase_day\") \\\n",
    "  .agg(expr(\"sum(total_amount) as daily_revenue\")) \\\n",
    "  .orderBy(\"purchase_day\") \\\n",
    "  .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ab92fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = df.toPandas()\n",
    "df_pd.to_csv(\"../output/retail_purchase_cleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
